{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import math\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "train_y_cls = np_utils.to_categorical(train_y,num_classes)\n",
    "test_y_cls = np_utils.to_categorical(test_y,num_classes)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(train_x)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "class_names = load_label_names()\n",
    "\n",
    "def label_to_array(cls_true):\n",
    "    true = cls_true.tolist()\n",
    "    true_label = []\n",
    "    for l in true:\n",
    "        true_label += l\n",
    "    return true_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Dimensions\n",
    "\n",
    "# CIFAR-10 images are 32 pixels in each dimension.\n",
    "img_size = 32\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images:\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Images\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    # Adjust vertical spacing if we need to print ensemble and best-net.\n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Interpolation type.\n",
    "        if smooth:\n",
    "            interpolation = 'spline16'\n",
    "        else:\n",
    "            interpolation = 'nearest'\n",
    "\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i, :, :, :],\n",
    "                  interpolation=interpolation)\n",
    "            \n",
    "        # Name of the true class.\n",
    "        cls_true_name = class_names[cls_true[i]]\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true_name)\n",
    "        else:\n",
    "            # Name of the predicted class.\n",
    "            cls_pred_name = class_names[cls_pred[i]]\n",
    "\n",
    "            xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 9 images from the test-set.\n",
    "images = train_x[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = train_y[0:9]\n",
    "cls_true = label_to_array(cls_true)\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images, cls_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise Data\n",
    "train_x = (train_x.astype('float32'))/255.\n",
    "test_x = (test_x.astype('float32'))/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5f5969c593d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m x_train, x_validation, y_train, y_validation = train_test_split(train_x, train_y_cls,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                                                 test_size=0.2, random_state=42)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(train_x, train_y_cls,\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_sequential(width, height, channels, classes):\n",
    "    # Initialize the model along with the input shape to be\n",
    "    # \"channels last\" ordering\n",
    "    # Note: in Alexnet, all paddings were 'valid'\n",
    "    # Here, we are taking the size of all filters as 3x3 and adding BatchNormalisation after each layer\n",
    "    \n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, channels)\n",
    "    \n",
    "    # Define the first CONV Layer\n",
    "    model.add(Conv2D(48, (3,3), # In Alexnet this kernel size is 11\n",
    "                     input_shape=inputShape,\n",
    "                     name='conv_layer1',\n",
    "                     dtype=tf.float32,\n",
    "                     strides=1, ## Originally: 4\n",
    "                     padding='same')) ## Orignally:'valid'\n",
    "    model.add(Activation(\"relu\", name='activation1'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2,\n",
    "                          padding=\"same\", name='maxpool1'))\n",
    "    model.add(BatchNormalization(name='BatchNorm1'))\n",
    "    \n",
    "    # Define the second CONV Layer\n",
    "    model.add(Conv2D(96, (3, 3),\n",
    "                     name='conv_layer2',\n",
    "                     dtype=tf.float32,\n",
    "                     padding='same')) \n",
    "    model.add(Activation(\"relu\", name='activation2'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2,\n",
    "                          padding='same', name='maxpool2'))\n",
    "    model.add(BatchNormalization(name='BatchNorm2'))\n",
    "    \n",
    "    # Define the third CONV Layer\n",
    "    model.add(Conv2D(192, (3, 3), padding='same',\n",
    "                     name='conv_layer3',\n",
    "                     dtype=tf.float32))\n",
    "    model.add(Activation(\"relu\", name='activation3'))\n",
    "    model.add(BatchNormalization(name='BatchNorm3'))\n",
    "    \n",
    "    # Define the fourth CONV Layer\n",
    "    model.add(Conv2D(192, (3, 3), padding='same',\n",
    "                     name='conv_layer4',\n",
    "                     dtype=tf.float32))\n",
    "    model.add(Activation(\"relu\", name='activation4'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2,\n",
    "                          padding='same', name='maxpool3'))\n",
    "    \n",
    "    model.add(BatchNormalization(name='BatchNorm4'))\n",
    "    \n",
    "    ## Define fifth CONV Layer\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',\n",
    "                     name='conv_layer5',\n",
    "                     dtype=tf.float32))\n",
    "    model.add(Activation(\"relu\", name='activation5'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2,\n",
    "                          padding='same', name='maxpool4'))\n",
    "    model.add(BatchNormalization(name='BatchNorm5'))\n",
    "    \n",
    "    ## Note: Only make 2/3 Conv layers as more will\n",
    "    ##       require a lot of computations\n",
    "    \n",
    "    # First Dense Layer with ReLU activation\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Second Dense Layer with ReLU activation\n",
    "    model.add(Dense(512, name='dense1'))\n",
    "    model.add(Activation(\"relu\", name='activation6'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Third Dense Layer with ReLU activation\n",
    "    model.add(Dense(256, name='dense2'))\n",
    "    model.add(Activation(\"relu\", name='activation7'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Final Dense Layer with softmax activation\n",
    "    model.add(Dense(num_classes, name='dense3'))\n",
    "    model.add(Activation(\"softmax\", name='activation8'))\n",
    "    \n",
    "    # return the constructed network architecture\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Model\n",
    "model = alexnet_sequential(32, 32, 3, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "# Initialize the optimizer and compile the model\n",
    "optimizer = Adam(lr=0.01, decay=0.0005 / num_epochs)\n",
    "print(\"Training network...\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmnetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Data Augmentation or not\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not augment_data:\n",
    "    print('Not using data augmentation.')\n",
    "    h = model.fit(x_train,\n",
    "                  y_train,\n",
    "                  batch_size = 128,\n",
    "                  validation_data=(x_validation,y_validation),\n",
    "                  epochs = num_epochs,\n",
    "                  verbose = 1)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
    "                        epochs=num_epochs, verbose=1, workers=4, \n",
    "                        validation_data=(x_validation,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on Test Set\n",
    "result = model.evaluate(test_x, test_y_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test_set\n",
    "pred_y = model.predict(test_x)\n",
    "pred_cls = np.argmax(pred_y, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some predicted Images\n",
    "# Get the first 9 images from the test-set.\n",
    "images = test_x[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = test_y[0:9]\n",
    "cls_true = label_to_array(cls_true)\n",
    "\n",
    "#Get Predicted Classes\n",
    "cls_pred = pred_cls[0:9]\n",
    "cls_pred = cls_pred.tolist()\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images, cls_true, cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
