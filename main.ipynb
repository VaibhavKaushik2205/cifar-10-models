{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import math\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import ResNetV1, ResNetV2, DenseNet, Cifar10, VGG, Inception, AlexNet  #import * not working properly\n",
    "from tf_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(train_data, train_labels), (test_x, test_y) = cifar10.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "train_labels_cls = np_utils.to_categorical(train_labels,num_classes)\n",
    "test_y_cls = np_utils.to_categorical(test_y,num_classes)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(train_data)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "class_names = load_label_names()\n",
    "\n",
    "def label_to_array(cls_true):\n",
    "    true = cls_true.tolist()\n",
    "    true_label = []\n",
    "    for l in true:\n",
    "        true_label += l\n",
    "    return true_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Dimensions\n",
    "\n",
    "# CIFAR-10 images are 32 pixels in each dimension.\n",
    "img_size = 32\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images:\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10\n",
    "\n",
    "# Define input shape of data\n",
    "input_shape = (img_size,img_size,num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 9 images from the train-set.\n",
    "images = train_data[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = train_labels[0:9]\n",
    "cls_true = label_to_array(cls_true)\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images, cls_true, class_names=class_names) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, validation_x, train_y, validation_y = train_test_split(train_data, train_labels_cls,\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Normalize text\n",
    "subtract_pixel_mean = True\n",
    "crop_images = False  # randomly crop batches of training data\n",
    "\n",
    "print ('Preparing data.....')\n",
    "train_x, validation_x, test_x = data_prep(train_x, validation_x, test_x, subtract_pixel_mean)\n",
    "\n",
    "if (crop_images):\n",
    "    train_x = augment_batches(train_x, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "model = Cifar10.cifar_model() # works better with Adam optimizer\n",
    "# model = AlexNet.alexnet()\n",
    "# model = VGG.vgg('VGG19')\n",
    "# model = ResNetV1.resnet(depth) # a multiple of 6n+2(e.g. 20, 32 etc.)\n",
    "# model = ResNetV2.resnet(depth) # a multiple of 9n+2(e.g. 56, 110 etc.)\n",
    "# model = Inception.inception()\n",
    "# model = DenseNet.densenet(model, growth_rate=12, compression=0.5)  #growth_rate can be 12,32,40\n",
    "                                                                     #(for cifar-10 use gr=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "# Initialize the optimizer and compile the model\n",
    "optimizer = SGD(lr=0.00, momentum=0.9, decay=5e-4)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decay Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decay function\n",
    "  \n",
    "def decay_fn(epoch, lr):\n",
    "    if epoch < int(num_epochs*0.5):\n",
    "        return 0.1\n",
    "    elif epoch >= int(num_epochs*0.5) and epoch < int(num_epochs*0.75):\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = tf.keras.callbacks.LearningRateScheduler(decay_fn)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with data augmentation\n",
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    h = model.fit(train_x, train_y,\n",
    "                  batch_size = 64,\n",
    "                  validation_data=(validation_x, validation_y),\n",
    "                  callbacks=callbacks_list, \n",
    "                  epochs = num_epochs,\n",
    "                  verbose = 1)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_x)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(train_x, train_y,\n",
    "                                     batch_size=64),\n",
    "                                     validation_data=(validation_x, validation_y),\n",
    "                                     epochs=num_epochs, verbose=1, workers=4,\n",
    "                                     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_x, test_y_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test_set\n",
    "pred_y = model.predict(test_x)\n",
    "pred_cls = np.argmax(pred_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some predicted Images\n",
    "# Get the first 9 images from the test-set.\n",
    "images = test_x[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = test_y[0:9]\n",
    "cls_true = label_to_array(cls_true)\n",
    "\n",
    "#Get Predicted Classes\n",
    "cls_pred = pred_cls[0:9]\n",
    "cls_pred = cls_pred.tolist()\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images, cls_true, cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
